{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "#### Different examples of tokenisation in Python.\n",
    "\n",
    "#### First, we need to import the nltk libraries and regular expression features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 41.5/41.5 kB 505.0 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.5 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 9.6 MB/s eta 0:00:00\n",
      "Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 273.5/273.5 kB 8.5 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.9.11 tqdm-4.66.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.14.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scipy) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycountry\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.7/6.3 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 14.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.0/6.3 MB 12.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.8/6.3 MB 14.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.5/6.3 MB 15.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 16.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 18.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pycountry\n",
      "Successfully installed pycountry-24.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package crubadan to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\crubadan.zip.\n"
     ]
    }
   ],
   "source": [
    "# first install the required packages\n",
    "!pip3 install nltk\n",
    "!pip3 install scipy\n",
    "!pip3 install numpy\n",
    "!pip3 install pycountry\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('crubadan')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's create some sample data to play with...\n",
    "\n",
    "#### I used used a couple of lines of text from a BBC sports page as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The European Super League (ESL) is on \"standby\" despite nine of the 12 founding ' \\\n",
    "       'teams withdrawing, says Real Madrid president Florentino Perez.  After a furious '\\\n",
    "       'backlash against the proposed tournament that was announced on Sunday, all six '\\\n",
    "       'Premier League clubs involved withdrew on Tuesday.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we can tokenise by word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will notice in the list above it treats punctuation as individual tokens.  It's easy enough to strip this out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [word for word in word_tokens if word.isalpha()]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Well, the punctuation is gone but notice the 12 disappeared?  That's because it's not an alpha character.  Let's try again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = [word for word in word_tokens if word.isalpha() or word.isnumeric()]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another quick and dirty way to tokenise is just to split on whitespace..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens2 = text.split()\n",
    "print(word_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the punctuation is now with the individual terms, we can still strip it out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [word for word in word_tokens2 if word.isalpha()]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ooops, that seems to have gotten rid of any text containing punctuation and the numerical value, let's try again. \n",
    "\n",
    "#### We will use a regular expression so that we end up just accepting words from the list.\n",
    "\n",
    "#### Note, the 2nd parameter says \"\" will be substituted if it does satisfy the regular expression.  You'll need to use this with care with some tokenisation as you may have punctuation on its own or another anomolies that will turn into \"\" entries in your list (which is easy enough to strip out at any rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens2 = [re.sub('[^\\w]', \"\", word) for word in word_tokens2]\n",
    "print(word_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That seems to have done the trick\n",
    "\n",
    "#### You will see below that the two tokenised versions are now the same, just 2 different ways of doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokens)\n",
    "print(word_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step will be to lower case the terms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = [word.lower() for word in word_tokens]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will also undoubtably come accross other cases, like contractions or other odditites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"O'Niell can't run.\"\n",
    "text = nltk.word_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, O'Niell comes out fine, but the contraction has been split up?  This may or may not be useful.\n",
    "\n",
    "#### Some NLP tools can deal with that type of input.  I generally avoid it which is one reason why splitting on space, in some cases, can make things easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"O'Niell can't run.\"\n",
    "text = text.split()\n",
    "print(text)\n",
    "text = [re.sub('[^\\w]', \"\", word) for word in text]\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatisation\n",
    "\n",
    "#### Let's run through some of the examples to see what happens if we stem/lemmatise them\n",
    "\n",
    "#### First, we need to import some additional nltk libraries to work with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the cases with punctuation we introduced above as well as some other \"gotcha\" cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem(\"O'Niell\"))\n",
    "print(porter_stemmer.stem(\"can't\"))\n",
    "print(porter_stemmer.stem(\"cant'\"))\n",
    "print(porter_stemmer.stem(\"hers'\"))\n",
    "print(porter_stemmer.stem(\"hers\"))\n",
    "print(porter_stemmer.stem(\"university\"))\n",
    "print(porter_stemmer.stem(\"universe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's stem the longer text we tokenized earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokens)\n",
    "stemmed_text = [porter_stemmer.stem(word) for word in word_tokens]\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's apply lemmatisation.  \n",
    "\n",
    "#### Remember, we need to the part of speech tag for this to work properly so let's get that first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTags = nltk.pos_tag(word_tokens)\n",
    "print(POSTags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we needed to look it up, we can actually print out the meanings of the POS if you don't know them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in POSTags:\n",
    "  nltk.help.upenn_tagset(t[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will need to make a translation function in order to use our POS in the WordNet Lemmatiser as it uses a different set (or subset) of POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_post(word):\n",
    "  # Remember, the word is a tuple, word[0] = word, word[1] = POS Tag\n",
    "  tag = word[1][0].upper()\n",
    "  tag_dictionary = { \"J\": wordnet.ADJ,\n",
    "\t                 \"N\": wordnet.NOUN,\n",
    "\t                 \"V\": wordnet.VERB,\n",
    "\t                 \"R\": wordnet.ADV}\n",
    "\t\n",
    "  # retrive value from dictionary, if not found use default of NOUN\n",
    "  return tag_dictionary.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can lemmatise our text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "print(\"Lemmatisation of the sentence: \")\n",
    "for t in POSTags:\n",
    "  term = t[0]\n",
    "  print(\"[\" + term + \"]:  \" + lemmatiser.lemmatize(term, pos = get_wordnet_post(t)) + \\\n",
    "        \" which is a \" + get_wordnet_post(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, using a lemmatiser is somewhat more work - in practice I find results between stemming/lemmatisation are usually pretty similar, not much to choose between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, an example of what happens if you *don't* use a POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"better :\", lemmatiser.lemmatize(\"better\", pos =\"a\"))\n",
    "print(\"better :\", lemmatiser.lemmatize(\"better\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection\n",
    "\n",
    "#### To cap things off, we will look at a model that guesses the language of text.  \n",
    "\n",
    "#### First, let's come up with some sample pieces of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "#English\n",
    "phrase_one = \"good morning\"\n",
    "# Afrikaans\n",
    "phrase_two = \"goeie more\"\n",
    "# Italian\n",
    "phrase_three = \"buongiorno\"\n",
    "# Korean\n",
    "phrase_four = \"좋은 아침\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we will instantiate a text classificaiton model and see what we come up with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = nltk.classify.textcat.TextCat() \n",
    "guess_one = tc.guess_language(phrase_one)\n",
    "guess_two = tc.guess_language(phrase_two)\n",
    "guess_three = tc.guess_language(phrase_three)\n",
    "guess_four = tc.guess_language(phrase_four)\n",
    "\n",
    "print(guess_one)\n",
    "print(guess_two)\n",
    "print(guess_three)\n",
    "print(guess_four)\n",
    "\n",
    "guess_one_name = pycountry.languages.get(alpha_3=guess_one).name\n",
    "guess_two_name = pycountry.languages.get(alpha_3=guess_two).name\n",
    "guess_three_name = pycountry.languages.get(alpha_3=guess_three).name\n",
    "guess_four_name = pycountry.languages.get(alpha_3=guess_four).name\n",
    "print(guess_one_name)\n",
    "print(guess_two_name)\n",
    "print(guess_three_name)\n",
    "print(guess_four_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
