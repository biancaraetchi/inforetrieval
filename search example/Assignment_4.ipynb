{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "### Question *a*\n",
    "The component $log(1 + f_{ij})$ has the role of reducing the impact of terms with a high frequency. By using the logarithm instead of the local weight directly, frequent, but less important terms will have less 'weight' in the computation.\n",
    "\n",
    "### Question *b*\n",
    "The second component is used as a constant for all of the calculations for all documents in which the term *i* appears and it describes the entropy of the state: the more evenly-distributed a term is across the documents in which it appears, the lower the value of this second component is (less 'weight'). This component's purpose is to ensure that more important terms in the document body will score higher, which is especially important for queries looking for documents with multiple terms, since it will let the algorithm know which term to prioritize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question *c*\n",
    "*{ see code }*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question *d*\n",
    "\n",
    "Compared to TF, the log entropy weighting works better: in TF, we're accounting only for the frequency of the words used, not taking into account their relevance according to the context, while log entropy weighting also takes their spread into account. \n",
    "\n",
    "Log entropy weighting's performance is comparable to TFIDF, however, it does have a drawback: if a term is encountered too often in a set of documents, the log weighting can disregard its usages as noise by giving it a very low weight, even though it is actually relevant, compared to TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "d:\\Uni work\\Information Retrieval\\inforetrieval\\search example\\inverted_index.py:185: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  t = [re.sub('[^\\w]', \"\", w) for w in t]\n"
     ]
    }
   ],
   "source": [
    "# first install the required packages\n",
    "# !pip3 install nltk\n",
    "# !pip3 install scipy\n",
    "# !pip3 install numpy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "from inverted_index import InvertedIndex\n",
    "from utils import read_data\n",
    "inv_ind = InvertedIndex()\n",
    "documents = read_data(\"./shakespeare\")\n",
    "for d in documents:\n",
    "    inv_ind.add_document(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.16118727379382877)\n",
      "('King Henry VI', 0.030048860936556884)\n",
      "('King Henry IV', 0.02948619616502898)\n",
      "('King Henry IV, II', 0.023764658494466812)\n",
      "('King Richard III', 0.01760253727154099)\n",
      "('King Henry V', 0.014994910138431003)\n",
      "('King John', 0.012133633289107127)\n",
      "('King Richard II', 0.012106985654825615)\n",
      "(\"All's Well that Ends Well\", 0.01119139253053665)\n",
      "('King Lear', 0.010978210515097522)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcLogEntropy()\n",
    "inv_ind.generate_term_by_doc_matrix(log_entropy = True)\n",
    "results = inv_ind.search(\"scotland kings and thanes\", log_entropy=True)\n",
    "\n",
    "for r in range(10):\n",
    "    print (results[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08559316237351267)\n",
      "('King Henry IV', 0.005789261723483593)\n",
      "('King Henry VI', 0.003660436049077642)\n",
      "('King Henry IV, II', 0.003121709934588564)\n",
      "('King Henry V', 0.0019193400093131976)\n",
      "('King Richard III', 0.0013431147327243318)\n",
      "('King John', 0.0007488196759316429)\n",
      "('King Richard II', 0.0006742482860831404)\n",
      "('King Henry VIII', 0.0005161221482695165)\n",
      "('The Comedy of Errors', 0.00046244490997942336)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcTFIDF()\n",
    "inv_ind.generate_term_by_doc_matrix(tfidf=True)\n",
    "results = inv_ind.search(\"scotland kings and thanes\", tfidf=True)\n",
    "\n",
    "for r in range(10):\n",
    "    print (results[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.06342271855923712)\n",
      "('King Henry VI', 0.01072781887182939)\n",
      "('King Henry V', 0.009657775892130944)\n",
      "('King John', 0.008469965122871384)\n",
      "('King Richard II', 0.0077213170531481206)\n",
      "('King Lear', 0.006993816919843515)\n",
      "('King Henry IV', 0.006848287703722218)\n",
      "('King Henry VIII', 0.0068475238333851225)\n",
      "('King Richard III', 0.006723494450381792)\n",
      "('King Henry IV, II', 0.005135632065177295)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.generate_term_by_doc_matrix()\n",
    "results = inv_ind.search(\"scotland kings and thanes\", tfidf=True)\n",
    "\n",
    "for r in range(10):\n",
    "    print (results[r])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
